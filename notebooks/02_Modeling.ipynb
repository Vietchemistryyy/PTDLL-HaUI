{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ü´Ä Heart Disease Prediction - Modeling\n",
                "\n",
                "Training Logistic Regression v·ªõi K-Fold Cross Validation s·ª≠ d·ª•ng PySpark."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üì¶ 1. Import Libraries & Initialize Spark"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "import os\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Setup PySpark\n",
                "if 'SPARK_HOME' in os.environ:\n",
                "    del os.environ['SPARK_HOME']\n",
                "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
                "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
                "\n",
                "from pyspark.sql import SparkSession\n",
                "from pyspark.ml.classification import LogisticRegression\n",
                "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
                "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
                "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "from sklearn.metrics import confusion_matrix, roc_curve, precision_recall_curve, auc\n",
                "\n",
                "# Style settings\n",
                "plt.style.use('seaborn-v0_8-whitegrid')\n",
                "sns.set_palette('husl')\n",
                "plt.rcParams['figure.figsize'] = (12, 6)\n",
                "plt.rcParams['font.size'] = 12\n",
                "\n",
                "# Initialize Spark\n",
                "spark = SparkSession.builder.appName('HeartDiseaseModeling').getOrCreate()\n",
                "print('‚úÖ Spark Session Created Successfully!')\n",
                "spark"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üìÇ 2. Load Processed Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load processed data\n",
                "data_path = '../data/processed/cardio_processed.parquet'\n",
                "\n",
                "if os.path.exists(data_path):\n",
                "    df = spark.read.parquet(data_path)\n",
                "    print(f'‚úÖ Loaded processed data: {df.count()} rows')\n",
                "else:\n",
                "    print('‚ö†Ô∏è Processed data not found, loading raw data...')\n",
                "    df = spark.read.csv('../data/raw/cardio_train.csv', header=True, sep=',', inferSchema=True)\n",
                "    \n",
                "    # Quick preprocessing\n",
                "    pdf = df.toPandas()\n",
                "    pdf['age_years'] = pdf['age'] / 365\n",
                "    pdf['bmi'] = pdf['weight'] / ((pdf['height'] / 100) ** 2)\n",
                "    pdf['pulse_pressure'] = pdf['ap_hi'] - pdf['ap_lo']\n",
                "    \n",
                "    df = spark.createDataFrame(pdf)\n",
                "    feature_cols = ['age_years', 'gender', 'height', 'weight', 'ap_hi', 'ap_lo', 'cholesterol', 'gluc', 'smoke', 'alco', 'active', 'bmi', 'pulse_pressure']\n",
                "    assembler = VectorAssembler(inputCols=feature_cols, outputCol='features')\n",
                "    df = assembler.transform(df)\n",
                "    scaler = StandardScaler(inputCol='features', outputCol='scaled_features', withStd=True, withMean=True)\n",
                "    df = scaler.fit(df).transform(df)\n",
                "    print(f'‚úÖ Preprocessed raw data: {df.count()} rows')\n",
                "\n",
                "df.select('scaled_features', 'cardio').show(5, truncate=False)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üìä 3. Train/Test Split"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Split data\n",
                "train_data, test_data = df.randomSplit([0.8, 0.2], seed=42)\n",
                "\n",
                "train_count = train_data.count()\n",
                "test_count = test_data.count()\n",
                "total = train_count + test_count\n",
                "\n",
                "print(f'üìä Training Data: {train_count:,} ({train_count/total*100:.1f}%)')\n",
                "print(f'üìä Test Data: {test_count:,} ({test_count/total*100:.1f}%)')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize split\n",
                "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
                "\n",
                "# Pie chart - Split ratio\n",
                "axes[0].pie([train_count, test_count], labels=['Train (80%)', 'Test (20%)'],\n",
                "            autopct='%1.1f%%', colors=['#3498db', '#e74c3c'], explode=(0.02, 0.02), shadow=True)\n",
                "axes[0].set_title('üìä Train/Test Split', fontsize=14, fontweight='bold')\n",
                "\n",
                "# Class distribution in train\n",
                "train_pdf = train_data.select('cardio').toPandas()\n",
                "test_pdf = test_data.select('cardio').toPandas()\n",
                "\n",
                "sns.countplot(x='cardio', data=train_pdf, ax=axes[1], palette=['#2ecc71', '#e74c3c'])\n",
                "axes[1].set_title('Training Set Class Distribution', fontsize=12, fontweight='bold')\n",
                "axes[1].set_xticklabels(['Healthy', 'Disease'])\n",
                "for i, v in enumerate(train_pdf['cardio'].value_counts().sort_index()):\n",
                "    axes[1].text(i, v + 200, f'{v:,}', ha='center', fontweight='bold')\n",
                "\n",
                "# Class distribution in test\n",
                "sns.countplot(x='cardio', data=test_pdf, ax=axes[2], palette=['#2ecc71', '#e74c3c'])\n",
                "axes[2].set_title('Test Set Class Distribution', fontsize=12, fontweight='bold')\n",
                "axes[2].set_xticklabels(['Healthy', 'Disease'])\n",
                "for i, v in enumerate(test_pdf['cardio'].value_counts().sort_index()):\n",
                "    axes[2].text(i, v + 50, f'{v:,}', ha='center', fontweight='bold')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('../results/12_train_test_split.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()\n",
                "print('‚úÖ Chart saved to ../results/12_train_test_split.png')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üéØ 4. Logistic Regression with K-Fold Cross Validation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define Logistic Regression\n",
                "lr = LogisticRegression(featuresCol='scaled_features', labelCol='cardio', maxIter=100)\n",
                "\n",
                "# Parameter Grid\n",
                "paramGrid = (ParamGridBuilder()\n",
                "             .addGrid(lr.regParam, [0.001, 0.01, 0.1])\n",
                "             .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\n",
                "             .build())\n",
                "\n",
                "print(f'üìä Total parameter combinations: {len(paramGrid)}')\n",
                "print('\\nüîß Parameters to tune:')\n",
                "print('  ‚Ä¢ regParam: [0.001, 0.01, 0.1]')\n",
                "print('  ‚Ä¢ elasticNetParam: [0.0, 0.5, 1.0]')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# K-Fold Cross Validator (K=5)\n",
                "evaluator = BinaryClassificationEvaluator(labelCol='cardio', metricName='areaUnderROC')\n",
                "\n",
                "cv = CrossValidator(\n",
                "    estimator=lr,\n",
                "    estimatorParamMaps=paramGrid,\n",
                "    evaluator=evaluator,\n",
                "    numFolds=5,\n",
                "    parallelism=2\n",
                ")\n",
                "\n",
                "print('üöÄ Training with 5-Fold Cross Validation...')\n",
                "print('‚è≥ This may take a few minutes...')\n",
                "\n",
                "cvModel = cv.fit(train_data)\n",
                "bestModel = cvModel.bestModel\n",
                "\n",
                "print('\\n‚úÖ Training completed!')\n",
                "print(f'\\nüèÜ Best Model Parameters:')\n",
                "print(f'  ‚Ä¢ regParam: {bestModel.getRegParam()}')\n",
                "print(f'  ‚Ä¢ elasticNetParam: {bestModel.getElasticNetParam()}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize Cross Validation Results\n",
                "avg_metrics = cvModel.avgMetrics\n",
                "\n",
                "# Create parameter labels\n",
                "param_labels = []\n",
                "for params in paramGrid:\n",
                "    reg = params[lr.regParam]\n",
                "    elastic = params[lr.elasticNetParam]\n",
                "    param_labels.append(f'reg={reg}\\nelastic={elastic}')\n",
                "\n",
                "fig, ax = plt.subplots(figsize=(14, 6))\n",
                "colors = plt.cm.viridis(np.linspace(0, 0.8, len(avg_metrics)))\n",
                "bars = ax.bar(range(len(avg_metrics)), avg_metrics, color=colors, edgecolor='black')\n",
                "ax.set_xticks(range(len(param_labels)))\n",
                "ax.set_xticklabels(param_labels, rotation=45, ha='right', fontsize=9)\n",
                "ax.set_xlabel('Parameter Combination')\n",
                "ax.set_ylabel('Average ROC-AUC (5-Fold CV)')\n",
                "ax.set_title('üìä 5-Fold Cross Validation Results', fontsize=14, fontweight='bold')\n",
                "ax.set_ylim([min(avg_metrics) - 0.02, max(avg_metrics) + 0.02])\n",
                "\n",
                "# Highlight best\n",
                "best_idx = avg_metrics.index(max(avg_metrics))\n",
                "bars[best_idx].set_color('#e74c3c')\n",
                "bars[best_idx].set_edgecolor('black')\n",
                "bars[best_idx].set_linewidth(2)\n",
                "\n",
                "for i, v in enumerate(avg_metrics):\n",
                "    ax.text(i, v + 0.002, f'{v:.4f}', ha='center', fontsize=8, fontweight='bold' if i == best_idx else 'normal')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('../results/13_cv_results.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()\n",
                "print('‚úÖ Chart saved to ../results/13_cv_results.png')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üìà 5. Model Evaluation on Test Set"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Make predictions\n",
                "predictions = bestModel.transform(test_data)\n",
                "\n",
                "# Calculate metrics\n",
                "accuracy = MulticlassClassificationEvaluator(labelCol='cardio', metricName='accuracy').evaluate(predictions)\n",
                "precision = MulticlassClassificationEvaluator(labelCol='cardio', metricName='weightedPrecision').evaluate(predictions)\n",
                "recall = MulticlassClassificationEvaluator(labelCol='cardio', metricName='weightedRecall').evaluate(predictions)\n",
                "f1 = MulticlassClassificationEvaluator(labelCol='cardio', metricName='f1').evaluate(predictions)\n",
                "roc_auc = BinaryClassificationEvaluator(labelCol='cardio', metricName='areaUnderROC').evaluate(predictions)\n",
                "pr_auc = BinaryClassificationEvaluator(labelCol='cardio', metricName='areaUnderPR').evaluate(predictions)\n",
                "\n",
                "print('üìä Model Evaluation Metrics:')\n",
                "print('=' * 40)\n",
                "print(f'  Accuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)')\n",
                "print(f'  Precision: {precision:.4f}')\n",
                "print(f'  Recall:    {recall:.4f}')\n",
                "print(f'  F1-Score:  {f1:.4f}')\n",
                "print(f'  ROC-AUC:   {roc_auc:.4f}')\n",
                "print(f'  PR-AUC:    {pr_auc:.4f}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize metrics\n",
                "fig, ax = plt.subplots(figsize=(10, 6))\n",
                "\n",
                "metrics_names = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC', 'PR-AUC']\n",
                "metrics_values = [accuracy, precision, recall, f1, roc_auc, pr_auc]\n",
                "colors = ['#3498db', '#2ecc71', '#e74c3c', '#9b59b6', '#f39c12', '#1abc9c']\n",
                "\n",
                "bars = ax.bar(metrics_names, metrics_values, color=colors, edgecolor='black', linewidth=1.5)\n",
                "ax.set_ylim([0, 1.1])\n",
                "ax.set_ylabel('Score')\n",
                "ax.set_title('üìä Model Evaluation Metrics', fontsize=14, fontweight='bold')\n",
                "ax.axhline(y=0.7, color='gray', linestyle='--', alpha=0.5, label='Baseline (0.7)')\n",
                "\n",
                "for bar, val in zip(bars, metrics_values):\n",
                "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, f'{val:.3f}', ha='center', fontweight='bold', fontsize=11)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('../results/14_evaluation_metrics.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()\n",
                "print('‚úÖ Chart saved to ../results/14_evaluation_metrics.png')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üéØ 6. Confusion Matrix"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get predictions as pandas\n",
                "pred_pdf = predictions.select('cardio', 'prediction').toPandas()\n",
                "y_true = pred_pdf['cardio'].values\n",
                "y_pred = pred_pdf['prediction'].values\n",
                "\n",
                "# Confusion Matrix\n",
                "cm = confusion_matrix(y_true, y_pred)\n",
                "\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# Heatmap - counts\n",
                "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
                "            xticklabels=['Healthy', 'Disease'], yticklabels=['Healthy', 'Disease'],\n",
                "            cbar_kws={'shrink': 0.8})\n",
                "axes[0].set_xlabel('Predicted')\n",
                "axes[0].set_ylabel('Actual')\n",
                "axes[0].set_title('üìä Confusion Matrix (Counts)', fontsize=12, fontweight='bold')\n",
                "\n",
                "# Heatmap - percentages\n",
                "cm_pct = cm.astype('float') / cm.sum() * 100\n",
                "sns.heatmap(cm_pct, annot=True, fmt='.1f', cmap='Greens', ax=axes[1],\n",
                "            xticklabels=['Healthy', 'Disease'], yticklabels=['Healthy', 'Disease'],\n",
                "            cbar_kws={'shrink': 0.8})\n",
                "axes[1].set_xlabel('Predicted')\n",
                "axes[1].set_ylabel('Actual')\n",
                "axes[1].set_title('üìä Confusion Matrix (Percentages %)', fontsize=12, fontweight='bold')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('../results/15_confusion_matrix.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()\n",
                "print('‚úÖ Chart saved to ../results/15_confusion_matrix.png')\n",
                "\n",
                "# Print detailed metrics\n",
                "tn, fp, fn, tp = cm.ravel()\n",
                "print(f'\\nüìä Detailed Metrics:')\n",
                "print(f'  True Negatives:  {tn:,}')\n",
                "print(f'  False Positives: {fp:,}')\n",
                "print(f'  False Negatives: {fn:,}')\n",
                "print(f'  True Positives:  {tp:,}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üìà 7. ROC Curve"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get probability scores\n",
                "from pyspark.sql.functions import udf\n",
                "from pyspark.sql.types import FloatType\n",
                "\n",
                "# Extract probability of positive class\n",
                "def extract_prob(v):\n",
                "    return float(v[1])\n",
                "\n",
                "extract_prob_udf = udf(extract_prob, FloatType())\n",
                "pred_with_prob = predictions.withColumn('prob_positive', extract_prob_udf('probability'))\n",
                "prob_pdf = pred_with_prob.select('cardio', 'prob_positive').toPandas()\n",
                "\n",
                "y_true = prob_pdf['cardio'].values\n",
                "y_scores = prob_pdf['prob_positive'].values\n",
                "\n",
                "# Calculate ROC curve\n",
                "fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
                "roc_auc_score = auc(fpr, tpr)\n",
                "\n",
                "fig, ax = plt.subplots(figsize=(10, 8))\n",
                "ax.plot(fpr, tpr, color='#e74c3c', lw=3, label=f'ROC Curve (AUC = {roc_auc_score:.4f})')\n",
                "ax.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--', label='Random Classifier')\n",
                "ax.fill_between(fpr, tpr, alpha=0.3, color='#e74c3c')\n",
                "ax.set_xlim([0.0, 1.0])\n",
                "ax.set_ylim([0.0, 1.05])\n",
                "ax.set_xlabel('False Positive Rate', fontsize=12)\n",
                "ax.set_ylabel('True Positive Rate', fontsize=12)\n",
                "ax.set_title('üìà ROC Curve', fontsize=14, fontweight='bold')\n",
                "ax.legend(loc='lower right', fontsize=11)\n",
                "ax.grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('../results/16_roc_curve.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()\n",
                "print('‚úÖ Chart saved to ../results/16_roc_curve.png')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üìà 8. Precision-Recall Curve"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calculate Precision-Recall curve\n",
                "precision_curve, recall_curve, thresholds_pr = precision_recall_curve(y_true, y_scores)\n",
                "pr_auc_score = auc(recall_curve, precision_curve)\n",
                "\n",
                "fig, ax = plt.subplots(figsize=(10, 8))\n",
                "ax.plot(recall_curve, precision_curve, color='#3498db', lw=3, label=f'PR Curve (AUC = {pr_auc_score:.4f})')\n",
                "ax.fill_between(recall_curve, precision_curve, alpha=0.3, color='#3498db')\n",
                "ax.set_xlim([0.0, 1.0])\n",
                "ax.set_ylim([0.0, 1.05])\n",
                "ax.set_xlabel('Recall', fontsize=12)\n",
                "ax.set_ylabel('Precision', fontsize=12)\n",
                "ax.set_title('üìà Precision-Recall Curve', fontsize=14, fontweight='bold')\n",
                "ax.legend(loc='lower left', fontsize=11)\n",
                "ax.grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('../results/17_precision_recall_curve.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()\n",
                "print('‚úÖ Chart saved to ../results/17_precision_recall_curve.png')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üìä 9. Feature Importance"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get feature coefficients\n",
                "coefficients = bestModel.coefficients.toArray()\n",
                "\n",
                "# Load feature names\n",
                "try:\n",
                "    with open('../data/processed/feature_columns.txt', 'r') as f:\n",
                "        feature_names = [line.strip() for line in f.readlines()]\n",
                "except:\n",
                "    feature_names = [f'Feature_{i}' for i in range(len(coefficients))]\n",
                "\n",
                "# Create DataFrame\n",
                "feature_importance = pd.DataFrame({\n",
                "    'Feature': feature_names,\n",
                "    'Coefficient': coefficients,\n",
                "    'Abs_Coefficient': np.abs(coefficients)\n",
                "}).sort_values('Abs_Coefficient', ascending=True)\n",
                "\n",
                "print('üìä Feature Coefficients:')\n",
                "for _, row in feature_importance.iterrows():\n",
                "    sign = '+' if row['Coefficient'] > 0 else '-'\n",
                "    print(f\"  {sign} {row['Feature']}: {row['Coefficient']:.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize feature importance\n",
                "fig, ax = plt.subplots(figsize=(12, 8))\n",
                "\n",
                "colors = ['#e74c3c' if c < 0 else '#2ecc71' for c in feature_importance['Coefficient']]\n",
                "ax.barh(feature_importance['Feature'], feature_importance['Coefficient'], color=colors, edgecolor='black')\n",
                "ax.set_xlabel('Coefficient Value')\n",
                "ax.set_title('üìä Feature Importance (Logistic Regression Coefficients)', fontsize=14, fontweight='bold')\n",
                "ax.axvline(x=0, color='black', linewidth=0.8)\n",
                "\n",
                "for i, (idx, row) in enumerate(feature_importance.iterrows()):\n",
                "    ax.text(row['Coefficient'] + 0.01 if row['Coefficient'] >= 0 else row['Coefficient'] - 0.01,\n",
                "            i, f\"{row['Coefficient']:.3f}\", va='center', ha='left' if row['Coefficient'] >= 0 else 'right', fontsize=9)\n",
                "\n",
                "# Add legend\n",
                "from matplotlib.patches import Patch\n",
                "legend_elements = [Patch(facecolor='#2ecc71', label='Positive (‚Üë Risk)'),\n",
                "                   Patch(facecolor='#e74c3c', label='Negative (‚Üì Risk)')]\n",
                "ax.legend(handles=legend_elements, loc='lower right')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('../results/18_feature_importance.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()\n",
                "print('‚úÖ Chart saved to ../results/18_feature_importance.png')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üìä 10. K-Fold Performance Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# K-Fold scores analysis\n",
                "cv_scores = cvModel.avgMetrics\n",
                "best_cv_score = max(cv_scores)\n",
                "mean_score = np.mean(cv_scores)\n",
                "std_score = np.std(cv_scores)\n",
                "\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# Box plot\n",
                "axes[0].boxplot(cv_scores, vert=True, patch_artist=True,\n",
                "                boxprops=dict(facecolor='#3498db', color='black'),\n",
                "                medianprops=dict(color='red', linewidth=2))\n",
                "axes[0].scatter([1], [best_cv_score], color='#e74c3c', s=100, zorder=5, label=f'Best: {best_cv_score:.4f}')\n",
                "axes[0].set_ylabel('ROC-AUC Score')\n",
                "axes[0].set_title('üì¶ Cross-Validation Scores Distribution', fontsize=12, fontweight='bold')\n",
                "axes[0].set_xticklabels(['5-Fold CV'])\n",
                "axes[0].legend()\n",
                "\n",
                "# Line plot\n",
                "axes[1].plot(range(1, len(cv_scores)+1), cv_scores, 'o-', color='#3498db', linewidth=2, markersize=8)\n",
                "axes[1].axhline(y=mean_score, color='#2ecc71', linestyle='--', linewidth=2, label=f'Mean: {mean_score:.4f}')\n",
                "axes[1].fill_between(range(1, len(cv_scores)+1), mean_score - std_score, mean_score + std_score, alpha=0.2, color='#2ecc71')\n",
                "axes[1].set_xlabel('Parameter Combination')\n",
                "axes[1].set_ylabel('ROC-AUC Score')\n",
                "axes[1].set_title('üìà CV Scores by Parameter Combination', fontsize=12, fontweight='bold')\n",
                "axes[1].legend()\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('../results/19_kfold_analysis.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()\n",
                "print('‚úÖ Chart saved to ../results/19_kfold_analysis.png')\n",
                "print(f'\\nüìä CV Statistics: Mean={mean_score:.4f}, Std={std_score:.4f}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üíæ 11. Save Model & Results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save the best model\n",
                "model_path = '../model/logistic_regression_model'\n",
                "bestModel.write().overwrite().save(model_path)\n",
                "print(f'‚úÖ Model saved to {model_path}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save model weights\n",
                "with open('../model/model_weights.txt', 'w') as f:\n",
                "    f.write('=' * 50 + '\\n')\n",
                "    f.write('LOGISTIC REGRESSION MODEL WEIGHTS\\n')\n",
                "    f.write('=' * 50 + '\\n\\n')\n",
                "    f.write(f'Intercept: {bestModel.intercept}\\n\\n')\n",
                "    f.write('Coefficients:\\n')\n",
                "    for i, (feat, coef) in enumerate(zip(feature_names, coefficients)):\n",
                "        f.write(f'  {feat}: {coef:.6f}\\n')\n",
                "    f.write('\\n' + '=' * 50 + '\\n')\n",
                "    f.write('HYPERPARAMETERS\\n')\n",
                "    f.write('=' * 50 + '\\n\\n')\n",
                "    f.write(f'regParam: {bestModel.getRegParam()}\\n')\n",
                "    f.write(f'elasticNetParam: {bestModel.getElasticNetParam()}\\n')\n",
                "\n",
                "print('‚úÖ Model weights saved to ../model/model_weights.txt')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save evaluation metrics\n",
                "metrics_dict = {\n",
                "    'Accuracy': accuracy,\n",
                "    'Precision': precision,\n",
                "    'Recall': recall,\n",
                "    'F1-Score': f1,\n",
                "    'ROC-AUC': roc_auc,\n",
                "    'PR-AUC': pr_auc\n",
                "}\n",
                "\n",
                "with open('../results/model_metrics.txt', 'w') as f:\n",
                "    f.write('=' * 40 + '\\n')\n",
                "    f.write('MODEL EVALUATION METRICS\\n')\n",
                "    f.write('=' * 40 + '\\n\\n')\n",
                "    for metric, value in metrics_dict.items():\n",
                "        f.write(f'{metric}: {value:.4f}\\n')\n",
                "    f.write('\\n' + '=' * 40 + '\\n')\n",
                "    f.write('CONFUSION MATRIX\\n')\n",
                "    f.write('=' * 40 + '\\n\\n')\n",
                "    f.write(f'True Negatives:  {tn:,}\\n')\n",
                "    f.write(f'False Positives: {fp:,}\\n')\n",
                "    f.write(f'False Negatives: {fn:,}\\n')\n",
                "    f.write(f'True Positives:  {tp:,}\\n')\n",
                "\n",
                "print('‚úÖ Metrics saved to ../results/model_metrics.txt')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üìä 12. Final Summary"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print('=' * 60)\n",
                "print('üéâ MODELING COMPLETED SUCCESSFULLY!')\n",
                "print('=' * 60)\n",
                "print(f'\\nüìä Model Performance:')\n",
                "print(f'  ‚Ä¢ Accuracy:  {accuracy*100:.2f}%')\n",
                "print(f'  ‚Ä¢ ROC-AUC:   {roc_auc:.4f}')\n",
                "print(f'  ‚Ä¢ F1-Score:  {f1:.4f}')\n",
                "print(f'\\nüèÜ Best Hyperparameters:')\n",
                "print(f'  ‚Ä¢ regParam: {bestModel.getRegParam()}')\n",
                "print(f'  ‚Ä¢ elasticNetParam: {bestModel.getElasticNetParam()}')\n",
                "print(f'\\nüíæ Saved Files:')\n",
                "print(f'  ‚Ä¢ model/logistic_regression_model/')\n",
                "print(f'  ‚Ä¢ model/model_weights.txt')\n",
                "print(f'  ‚Ä¢ results/model_metrics.txt')\n",
                "print(f'  ‚Ä¢ results/*.png (8 visualization charts)')\n",
                "print('\\n‚úÖ Ready for deployment!')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Stop Spark\n",
                "print('üí° Run spark.stop() when done to release resources')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## üìå Charts Generated\n",
                "\n",
                "| # | Chart | File |\n",
                "|---|-------|------|\n",
                "| 12 | Train/Test Split | 12_train_test_split.png |\n",
                "| 13 | CV Results | 13_cv_results.png |\n",
                "| 14 | Evaluation Metrics | 14_evaluation_metrics.png |\n",
                "| 15 | Confusion Matrix | 15_confusion_matrix.png |\n",
                "| 16 | ROC Curve | 16_roc_curve.png |\n",
                "| 17 | Precision-Recall Curve | 17_precision_recall_curve.png |\n",
                "| 18 | Feature Importance | 18_feature_importance.png |\n",
                "| 19 | K-Fold Analysis | 19_kfold_analysis.png |"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.12.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
