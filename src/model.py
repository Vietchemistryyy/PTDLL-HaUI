"""
Module Training vÃ  Evaluation cho Model Logistic Regression
Sá»­ dá»¥ng PySpark MLlib vá»›i K-fold Cross Validation Pipeline
"""
from pyspark.sql import SparkSession, DataFrame
from pyspark.ml.classification import LogisticRegression, LogisticRegressionModel
from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator
from pyspark.ml import Pipeline, PipelineModel
from pyspark.ml.tuning import ParamGridBuilder, CrossValidator
from pyspark.sql import functions as F
import pandas as pd
import logging
from pathlib import Path
import config

logger = logging.getLogger(__name__)


class CardioLogisticModel:
    """Class training Logistic Regression model cho dá»± Ä‘oÃ¡n bá»‡nh tim máº¡ch"""
    
    def __init__(self, spark: SparkSession):
        self.spark = spark
        self.model = None
        self.pipeline_model = None
        self.feature_names = None
        self.train_df = None
        self.test_df = None
        self.best_params = None
        
    def create_logistic_model(self, 
                             max_iter: int = 100,
                             reg_param: float = 0.01,
                             elastic_net_param: float = 0.0) -> LogisticRegression:
        """
        Táº¡o Logistic Regression model
        
        Args:
            max_iter: Sá»‘ iteration tá»‘i Ä‘a
            reg_param: Regularization parameter
            elastic_net_param: ElasticNet mixing parameter (0=L2, 1=L1)
            
        Returns:
            LogisticRegression model
        """
        logger.info("Táº¡o Logistic Regression model...")
        logger.info(f"  - Max iterations: {max_iter}")
        logger.info(f"  - Regularization: {reg_param}")
        logger.info(f"  - ElasticNet: {elastic_net_param}")
        
        lr = LogisticRegression(
            featuresCol="features",
            labelCol="cardio",
            maxIter=max_iter,
            regParam=reg_param,
            elasticNetParam=elastic_net_param,
            family="binomial"
        )
        
        return lr
    
    def split_data(self, df: DataFrame, train_ratio: float = 0.8, seed: int = 42):
        """
        BÆ¯á»šC 1: Chia dá»¯ liá»‡u Train/Test
        
        Args:
            df: DataFrame vá»›i features vÃ  label
            train_ratio: Tá»· lá»‡ train (default 0.8)
            seed: Random seed
        """
        logger.info("=" * 70)
        logger.info("BÆ¯á»šC 1: CHIA Dá»® LIá»†U TRAIN/TEST")
        logger.info("=" * 70)
        
        self.train_df, self.test_df = df.randomSplit([train_ratio, 1 - train_ratio], seed=seed)
        
        train_count = self.train_df.count()
        test_count = self.test_df.count()
        total_count = train_count + test_count
        
        # Kiá»ƒm tra phÃ¢n bá»‘ class trong train/test
        train_pos = self.train_df.filter("cardio = 1").count()
        train_neg = train_count - train_pos
        test_pos = self.test_df.filter("cardio = 1").count()
        test_neg = test_count - test_pos
        
        logger.info(f"\nðŸ“Š THá»NG KÃŠ PHÃ‚N CHIA:")
        logger.info(f"  Total:     {total_count:,} samples")
        logger.info(f"  Train:     {train_count:,} samples ({train_count/total_count*100:.1f}%)")
        logger.info(f"  Test:      {test_count:,} samples ({test_count/total_count*100:.1f}%)")
        logger.info(f"\n  Train - Positive: {train_pos:,} ({train_pos/train_count*100:.1f}%)")
        logger.info(f"  Train - Negative: {train_neg:,} ({train_neg/train_count*100:.1f}%)")
        logger.info(f"  Test  - Positive: {test_pos:,} ({test_pos/test_count*100:.1f}%)")
        logger.info(f"  Test  - Negative: {test_neg:,} ({test_neg/test_count*100:.1f}%)")
        
        logger.info("\nâœ“ Test set Ä‘Æ°á»£c giá»¯ nguyÃªn - KHÃ”NG Ä‘á»™ng Ä‘áº¿n cho Ä‘áº¿n evaluation cuá»‘i")
        logger.info("=" * 70)
        
        return self.train_df, self.test_df
    
    def train(self, 
             train_df: DataFrame,
             max_iter: int = 100,
             reg_param: float = 0.01,
             elastic_net_param: float = 0.0) -> LogisticRegressionModel:
        """
        Training model (training thá»§ cÃ´ng - khÃ´ng dÃ¹ng CV)
        
        Args:
            train_df: Training DataFrame (pháº£i cÃ³ cá»™t 'features' vÃ  'cardio')
            max_iter: Sá»‘ iteration tá»‘i Ä‘a
            reg_param: Regularization parameter
            elastic_net_param: ElasticNet mixing parameter
            
        Returns:
            Trained model
        """
        logger.info("=" * 60)
        logger.info("TRAINING MODEL (Manual - khÃ´ng dÃ¹ng CV)")
        logger.info("=" * 60)
        
        # Táº¡o model
        lr = self.create_logistic_model(max_iter, reg_param, elastic_net_param)
        
        # Training
        logger.info(f"Training vá»›i {train_df.count():,} samples...")
        self.model = lr.fit(train_df)
        
        logger.info("âœ“ Training hoÃ n thÃ nh!")
        logger.info(f"âœ“ Sá»‘ coefficients: {len(self.model.coefficients)}")
        
        return self.model
    
    def cross_validate(self, 
                      train_df: DataFrame,
                      param_grid: dict = None,
                      num_folds: int = 5) -> tuple:
        """
        BÆ¯á»šC 2: K-fold Cross Validation trÃªn TRAIN set Ä‘á»ƒ tÃ¬m best hyperparameters
        
        Args:
            train_df: Training DataFrame (CHá»ˆ train set, khÃ´ng bao gá»“m test)
            param_grid: Dictionary vá»›i cÃ¡c parameters cáº§n test
            num_folds: Sá»‘ folds cho cross validation
            
        Returns:
            Tuple (best_model, best_params, avg_metrics)
        """
        logger.info("=" * 70)
        logger.info(f"BÆ¯á»šC 2: K-FOLD CROSS VALIDATION ({num_folds} folds)")
        logger.info("=" * 70)
        logger.info("âš ï¸  CHÃš Ã: CV chá»‰ cháº¡y trÃªn TRAIN set, test set KHÃ”NG Ä‘Æ°á»£c Ä‘á»™ng Ä‘áº¿n!")
        
        # Táº¡o base model
        lr = LogisticRegression(
            featuresCol="features",
            labelCol="cardio",
            family="binomial"
        )
        
        # Param grid
        if param_grid is None:
            param_grid = {
                'maxIter': [50, 100, 150],
                'regParam': [0.001, 0.01, 0.1],
                'elasticNetParam': [0.0, 0.5, 1.0]
            }
        
        logger.info(f"\nðŸ” Testing parameters:")
        for key, values in param_grid.items():
            logger.info(f"  {key}: {values}")
        
        # Build param grid
        paramGridBuilder = ParamGridBuilder()
        for param_name, param_values in param_grid.items():
            if param_name == 'maxIter':
                paramGridBuilder = paramGridBuilder.addGrid(lr.maxIter, param_values)
            elif param_name == 'regParam':
                paramGridBuilder = paramGridBuilder.addGrid(lr.regParam, param_values)
            elif param_name == 'elasticNetParam':
                paramGridBuilder = paramGridBuilder.addGrid(lr.elasticNetParam, param_values)
        
        paramGrid = paramGridBuilder.build()
        
        logger.info(f"  â†’ Tá»•ng sá»‘ combinations: {len(paramGrid)}")
        
        # Evaluator
        evaluator = BinaryClassificationEvaluator(
            labelCol="cardio",
            rawPredictionCol="rawPrediction",
            metricName="areaUnderROC"
        )
        
        # Cross Validator
        cv = CrossValidator(
            estimator=lr,
            estimatorParamMaps=paramGrid,
            evaluator=evaluator,
            numFolds=num_folds,
            parallelism=2,
            seed=42
        )
        
        logger.info(f"\nðŸš€ Running {num_folds}-fold cross validation...")
        logger.info(f"   Training {len(paramGrid)} models Ã— {num_folds} folds = {len(paramGrid) * num_folds} total fits")
        
        cv_model = cv.fit(train_df)
        
        # Best model
        best_model = cv_model.bestModel
        
        # Best params
        best_params = {
            'maxIter': best_model.getMaxIter(),
            'regParam': best_model.getRegParam(),
            'elasticNetParam': best_model.getElasticNetParam()
        }
        
        # CV results
        avg_metrics = cv_model.avgMetrics
        best_auc = max(avg_metrics)
        
        logger.info("\nâœ“ Cross validation hoÃ n thÃ nh!")
        logger.info(f"\nðŸ“Š BEST PARAMETERS:")
        for key, value in best_params.items():
            logger.info(f"  {key}: {value}")
        logger.info(f"\nðŸ“ˆ BEST CV AUC: {best_auc:.4f}")
        logger.info(f"   (Trung bÃ¬nh trÃªn {num_folds} folds)")
        
        logger.info("=" * 70)
        
        self.model = best_model
        self.best_params = best_params
        
        return best_model, best_params, avg_metrics
    
    def train_with_cv_pipeline(self, 
                               df: DataFrame,
                               train_ratio: float = 0.8,
                               param_grid: dict = None,
                               num_folds: int = 5,
                               seed: int = 42) -> dict:
        """
        PIPELINE Äáº¦Y Äá»¦: Split â†’ K-fold CV â†’ Evaluate
        
        Workflow Ä‘Ãºng chuáº©n:
        1. Split data thÃ nh Train/Test (80/20)
        2. K-fold CV trÃªn TRAIN set â†’ tÃ¬m best hyperparameters
        3. Model tá»‘t nháº¥t Ä‘Ã£ Ä‘Æ°á»£c train trÃªn full train set trong CV
        4. Evaluate trÃªn TEST set (unseen data)
        
        Args:
            df: DataFrame Ä‘áº§y Ä‘á»§ vá»›i features vÃ  label
            train_ratio: Tá»· lá»‡ train/test (default 0.8)
            param_grid: Dict hyperparameters Ä‘á»ƒ test
            num_folds: Sá»‘ folds cho CV (default 5)
            seed: Random seed
            
        Returns:
            Dict chá»©a: train_df, test_df, best_model, best_params, cv_metrics, test_metrics
        """
        logger.info("\n" + "=" * 70)
        logger.info("ðŸš€ PIPELINE TRAINING Äáº¦Y Äá»¦ Vá»šI K-FOLD CROSS VALIDATION")
        logger.info("=" * 70)
        
        # BÆ°á»›c 1: Split train/test
        train_df, test_df = self.split_data(df, train_ratio=train_ratio, seed=seed)
        
        # BÆ°á»›c 2: K-fold CV trÃªn TRAIN set
        best_model, best_params, cv_results = self.cross_validate(
            train_df=train_df,
            param_grid=param_grid,
            num_folds=num_folds
        )
        
        # BÆ°á»›c 3: Evaluate trÃªn TEST set (unseen)
        logger.info("\n" + "=" * 70)
        logger.info("BÆ¯á»šC 3: ÄÃNH GIÃ CUá»I CÃ™NG TRÃŠN TEST SET")
        logger.info("=" * 70)
        logger.info("ðŸ“Š ÄÃ¢y lÃ  láº§n Äáº¦U TIÃŠN model nhÃ¬n tháº¥y test data!")
        
        test_metrics = self.evaluate(test_df)
        
        # Tá»•ng káº¿t
        logger.info("\n" + "=" * 70)
        logger.info("âœ… HOÃ€N THÃ€NH PIPELINE")
        logger.info("=" * 70)
        logger.info(f"\nðŸ“‹ SUMMARY:")
        logger.info(f"  Train samples: {train_df.count():,}")
        logger.info(f"  Test samples:  {test_df.count():,}")
        logger.info(f"  Best CV AUC:   {max(cv_results):.4f}")
        logger.info(f"  Test AUC:      {test_metrics['auc_roc']:.4f}")
        logger.info(f"  Test Accuracy: {test_metrics['accuracy']:.4f}")
        logger.info(f"  Test F1:       {test_metrics['f1_score']:.4f}")
        logger.info("=" * 70 + "\n")
        
        return {
            'train_df': train_df,
            'test_df': test_df,
            'best_model': best_model,
            'best_params': best_params,
            'cv_metrics': {
                'best_auc': max(cv_results),
                'all_auc_scores': cv_results,
                'num_folds': num_folds
            },
            'test_metrics': test_metrics
        }
    
    def predict(self, df: DataFrame) -> DataFrame:
        """
        Dá»± Ä‘oÃ¡n trÃªn DataFrame
        
        Args:
            df: DataFrame cáº§n dá»± Ä‘oÃ¡n (pháº£i cÃ³ cá»™t 'features')
            
        Returns:
            DataFrame vá»›i cá»™t prediction vÃ  probability
        """
        if self.model is None:
            raise ValueError("Model chÆ°a Ä‘Æ°á»£c training! HÃ£y gá»i train() hoáº·c train_with_cv_pipeline() trÆ°á»›c.")
        
        logger.info("Äang dá»± Ä‘oÃ¡n...")
        predictions = self.model.transform(df)
        
        return predictions
    
    def evaluate(self, test_df: DataFrame) -> dict:
        """
        ÄÃ¡nh giÃ¡ model trÃªn test set
        
        Args:
            test_df: Test DataFrame
            
        Returns:
            Dictionary chá»©a cÃ¡c metrics
        """
        logger.info("\nðŸ“Š ÄÃNH GIÃ MODEL")
        logger.info("-" * 70)
        
        # Dá»± Ä‘oÃ¡n
        predictions = self.predict(test_df)
        
        # Binary Classification Evaluator
        binary_evaluator = BinaryClassificationEvaluator(
            labelCol="cardio",
            rawPredictionCol="rawPrediction",
            metricName="areaUnderROC"
        )
        
        auc = binary_evaluator.evaluate(predictions)
        
        # Multiclass Evaluator cho accuracy, precision, recall, f1
        mc_evaluator = MulticlassClassificationEvaluator(
            labelCol="cardio",
            predictionCol="prediction"
        )
        
        accuracy = mc_evaluator.evaluate(predictions, {mc_evaluator.metricName: "accuracy"})
        precision = mc_evaluator.evaluate(predictions, {mc_evaluator.metricName: "weightedPrecision"})
        recall = mc_evaluator.evaluate(predictions, {mc_evaluator.metricName: "weightedRecall"})
        f1 = mc_evaluator.evaluate(predictions, {mc_evaluator.metricName: "f1"})
        
        # Confusion Matrix
        confusion_matrix = self.get_confusion_matrix(predictions)
        
        metrics = {
            'accuracy': round(accuracy, 4),
            'precision': round(precision, 4),
            'recall': round(recall, 4),
            'f1_score': round(f1, 4),
            'auc_roc': round(auc, 4),
            'confusion_matrix': confusion_matrix
        }
        
        # Log metrics
        logger.info(f"  Accuracy:  {metrics['accuracy']:.4f}")
        logger.info(f"  Precision: {metrics['precision']:.4f}")
        logger.info(f"  Recall:    {metrics['recall']:.4f}")
        logger.info(f"  F1-Score:  {metrics['f1_score']:.4f}")
        logger.info(f"  AUC-ROC:   {metrics['auc_roc']:.4f}")
        logger.info(f"\n  Confusion Matrix:")
        logger.info(f"    TN={confusion_matrix['TN']:,}  FP={confusion_matrix['FP']:,}")
        logger.info(f"    FN={confusion_matrix['FN']:,}  TP={confusion_matrix['TP']:,}")
        logger.info("-" * 70)
        
        return metrics
    
    def get_confusion_matrix(self, predictions: DataFrame) -> dict:
        """
        TÃ­nh confusion matrix
        
        Args:
            predictions: DataFrame vá»›i predictions
            
        Returns:
            Dictionary vá»›i TN, FP, FN, TP
        """
        tp = predictions.filter((F.col("prediction") == 1) & (F.col("cardio") == 1)).count()
        tn = predictions.filter((F.col("prediction") == 0) & (F.col("cardio") == 0)).count()
        fp = predictions.filter((F.col("prediction") == 1) & (F.col("cardio") == 0)).count()
        fn = predictions.filter((F.col("prediction") == 0) & (F.col("cardio") == 1)).count()
        
        return {
            'TP': tp,
            'TN': tn,
            'FP': fp,
            'FN': fn
        }
    
    def get_feature_importance(self, feature_names: list = None) -> pd.DataFrame:
        """
        Láº¥y feature importance tá»« coefficients
        
        Args:
            feature_names: List tÃªn cÃ¡c features
            
        Returns:
            Pandas DataFrame vá»›i feature importance
        """
        if self.model is None:
            raise ValueError("Model chÆ°a Ä‘Æ°á»£c training!")
        
        logger.info("TÃ­nh feature importance...")
        
        coefficients = self.model.coefficients.toArray()
        
        if feature_names is None:
            feature_names = [f"feature_{i}" for i in range(len(coefficients))]
        
        # Táº¡o DataFrame
        importance_df = pd.DataFrame({
            'feature': feature_names,
            'coefficient': coefficients,
            'abs_coefficient': abs(coefficients)
        })
        
        # Sort by absolute coefficient
        importance_df = importance_df.sort_values('abs_coefficient', ascending=False)
        
        logger.info(f"âœ“ Top 5 features quan trá»ng nháº¥t:")
        for idx, row in importance_df.head(5).iterrows():
            logger.info(f"  {row['feature']}: {row['coefficient']:.4f}")
        
        return importance_df
    
    def save_model(self, path: str = None):
        """
        LÆ°u model
        
        Args:
            path: ÄÆ°á»ng dáº«n lÆ°u model
        """
        if self.model is None:
            raise ValueError("Model chÆ°a Ä‘Æ°á»£c training!")
        
        if path is None:
            path = str(config.MODEL_PATH)
        
        logger.info(f"LÆ°u model táº¡i: {path}")
        
        # Táº¡o thÆ° má»¥c náº¿u chÆ°a cÃ³
        Path(path).parent.mkdir(parents=True, exist_ok=True)
        
        # Save model
        self.model.write().overwrite().save(path)
        
        logger.info("âœ“ ÄÃ£ lÆ°u model")
    
    def load_model(self, path: str = None):
        """
        Load model Ä‘Ã£ lÆ°u
        
        Args:
            path: ÄÆ°á»ng dáº«n model
        """
        if path is None:
            path = str(config.MODEL_PATH)
        
        logger.info(f"Load model tá»«: {path}")
        
        self.model = LogisticRegressionModel.load(path)
        
        logger.info("âœ“ ÄÃ£ load model")
        
        return self.model
    
    def get_prediction_probabilities(self, predictions: DataFrame) -> DataFrame:
        """
        Láº¥y probabilities cá»§a predictions
        
        Args:
            predictions: DataFrame vá»›i predictions
            
        Returns:
            DataFrame vá»›i probability cho má»—i class
        """
        from pyspark.ml.functions import vector_to_array
        
        predictions = predictions.withColumn(
            "prob_array",
            vector_to_array("probability")
        )
        
        predictions = predictions.withColumn(
            "prob_negative",
            F.col("prob_array")[0]
        ).withColumn(
            "prob_positive",
            F.col("prob_array")[1]
        )
        
        return predictions.select(
            "cardio",
            "prediction",
            "prob_negative",
            "prob_positive"
        )
    
    def predict_single(self, features: list) -> dict:
        """
        Dá»± Ä‘oÃ¡n cho má»™t sample Ä‘Æ¡n
        
        Args:
            features: List cÃ¡c giÃ¡ trá»‹ features
            
        Returns:
            Dictionary vá»›i prediction vÃ  probability
        """
        if self.model is None:
            raise ValueError("Model chÆ°a Ä‘Æ°á»£c training!")
        
        from pyspark.ml.linalg import Vectors
        
        # Táº¡o DataFrame tá»« features
        feature_vector = Vectors.dense(features)
        df = self.spark.createDataFrame(
            [(feature_vector,)],
            ["features"]
        )
        
        # Predict
        prediction = self.model.transform(df)
        
        # Extract results
        result = prediction.select("prediction", "probability").collect()[0]
        
        prob_array = result["probability"].toArray()
        
        return {
            'prediction': int(result["prediction"]),
            'probability_negative': float(prob_array[0]),
            'probability_positive': float(prob_array[1]),
            'prediction_label': 'CÃ³ nguy cÆ¡ bá»‡nh tim' if result["prediction"] == 1 else 'KhÃ´ng cÃ³ nguy cÆ¡'
        }
    
    def get_model_summary(self) -> dict:
        """
        Láº¥y summary cá»§a model
        
        Returns:
            Dictionary chá»©a model summary
        """
        if self.model is None:
            raise ValueError("Model chÆ°a Ä‘Æ°á»£c training!")
        
        summary = self.model.summary
        
        return {
            'total_iterations': summary.totalIterations,
            'objective_history': summary.objectiveHistory,
            'area_under_roc': summary.areaUnderROC,
            'features_count': len(self.model.coefficients)
        }


# ============================================================================
# HELPER FUNCTIONS
# ============================================================================

def create_and_train_model(train_df: DataFrame, 
                          test_df: DataFrame,
                          max_iter: int = 100,
                          reg_param: float = 0.01,
                          save_model: bool = True) -> tuple:
    """
    HÃ m tiá»‡n Ã­ch Ä‘á»ƒ táº¡o, train vÃ  evaluate model (Manual - khÃ´ng dÃ¹ng CV)
    
    Args:
        train_df: Training DataFrame
        test_df: Test DataFrame
        max_iter: Max iterations
        reg_param: Regularization parameter
        save_model: LÆ°u model hay khÃ´ng
        
    Returns:
        Tuple (model, metrics)
    """
    from src.utils import SparkManager
    
    spark = SparkManager.get_spark()
    
    # Táº¡o model
    cardio_model = CardioLogisticModel(spark)
    
    # Train
    cardio_model.train(train_df, max_iter=max_iter, reg_param=reg_param)
    
    # Evaluate
    metrics = cardio_model.evaluate(test_df)
    
    # Save
    if save_model:
        cardio_model.save_model()
    
    return cardio_model, metrics


def train_with_cv(df: DataFrame,
                  param_grid: dict = None,
                  num_folds: int = 5,
                  train_ratio: float = 0.8,
                  save_model: bool = True) -> dict:
    """
    HÃ m tiá»‡n Ã­ch Ä‘á»ƒ train vá»›i K-fold CV pipeline Ä‘áº§y Ä‘á»§
    
    Args:
        df: DataFrame Ä‘áº§y Ä‘á»§ vá»›i features vÃ  label
        param_grid: Dict hyperparameters Ä‘á»ƒ test
        num_folds: Sá»‘ folds cho CV
        train_ratio: Tá»· lá»‡ train/test
        save_model: LÆ°u model hay khÃ´ng
        
    Returns:
        Dict vá»›i Ä‘áº§y Ä‘á»§ káº¿t quáº£
    """
    from src.utils import SparkManager
    
    spark = SparkManager.get_spark()
    
    # Táº¡o model
    cardio_model = CardioLogisticModel(spark)
    
    # Cháº¡y pipeline Ä‘áº§y Ä‘á»§
    results = cardio_model.train_with_cv_pipeline(
        df=df,
        train_ratio=train_ratio,
        param_grid=param_grid,
        num_folds=num_folds
    )
    
    # Save model
    if save_model:
        cardio_model.save_model()
    
    results['model_instance'] = cardio_model
    
    return results