"""
COMPLETE PIPELINE - Ch·∫°y to√†n b·ªô project t·ª´ ƒë·∫ßu ƒë·∫øn cu·ªëi
File: run_complete_pipeline.py

Th·ª© t·ª±:
1. Load raw data
2. Preprocessing
3. Feature Engineering
4. Train v·ªõi K-fold CV
5. Evaluate & Save
"""

import sys
import logging

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Import c√°c modules
from src.utils import SparkManager
from src.data_preprocessing import CardioDataLoader, CardioDataPreprocessor
from src.feature_engineering import CardioFeatureEngineer
from src.model import CardioLogisticModel
import config


def main():
    """Pipeline ƒë·∫ßy ƒë·ªß t·ª´ A-Z"""
    
    print("\n" + "=" * 80)
    print("üöÄ CARDIOVASCULAR DISEASE PREDICTION - COMPLETE PIPELINE")
    print("=" * 80)
    
    # ========================================================================
    # B∆Ø·ªöC 0: Kh·ªüi t·∫°o Spark
    # ========================================================================
    print("\n" + "=" * 80)
    print("B∆Ø·ªöC 0: KH·ªûI T·∫†O SPARK")
    print("=" * 80)
    
    spark = SparkManager.get_spark(
        app_name=config.SPARK_CONFIG['app_name'],
        master=config.SPARK_CONFIG['master'],
        driver_memory=config.SPARK_CONFIG['driver_memory'],
        executor_memory=config.SPARK_CONFIG['executor_memory']
    )
    
    # ========================================================================
    # B∆Ø·ªöC 1: Load d·ªØ li·ªáu RAW
    # ========================================================================
    print("\n" + "=" * 80)
    print("B∆Ø·ªöC 1: LOAD D·ªÆ LI·ªÜU RAW")
    print("=" * 80)
    
    loader = CardioDataLoader(spark)
    df_raw = loader.load_data()
    
    logger.info(f"‚úì ƒê√£ load {df_raw.count():,} records")
    logger.info(f"‚úì S·ªë c·ªôt: {len(df_raw.columns)}")
    
    # Xem sample
    print("\nüìä Sample d·ªØ li·ªáu (5 d√≤ng ƒë·∫ßu):")
    df_raw.show(5, truncate=False)
    
    # ========================================================================
    # B∆Ø·ªöC 2: PREPROCESSING
    # ========================================================================
    print("\n" + "=" * 80)
    print("B∆Ø·ªöC 2: TI·ªÄN X·ª¨ L√ù D·ªÆ LI·ªÜU")
    print("=" * 80)
    
    preprocessor = CardioDataPreprocessor(spark)
    df_processed = preprocessor.preprocess_pipeline(df_raw)
    
    logger.info(f"‚úì D·ªØ li·ªáu sau preprocessing: {df_processed.count():,} records")
    
    # Xem c√°c c·ªôt m·ªõi
    new_cols = [col for col in df_processed.columns if col not in df_raw.columns]
    logger.info(f"‚úì C√°c c·ªôt m·ªõi ƒë∆∞·ª£c t·∫°o: {', '.join(new_cols)}")
    
    # ========================================================================
    # B∆Ø·ªöC 3: FEATURE ENGINEERING
    # ========================================================================
    print("\n" + "=" * 80)
    print("B∆Ø·ªöC 3: FEATURE ENGINEERING")
    print("=" * 80)
    
    feature_engineer = CardioFeatureEngineer(spark)
    
    # Chu·∫©n b·ªã features cho ML
    df_ml = feature_engineer.prepare_features_for_ml(df_processed)
    
    # T·∫°o interaction features
    df_ml = feature_engineer.create_interaction_features(df_ml)
    
    # T·∫°o binned features
    df_ml = feature_engineer.create_binned_features(df_ml)
    
    logger.info(f"‚úì ƒê√£ t·∫°o engineered features")
    
    # ========================================================================
    # B∆Ø·ªöC 4: PREPARE FEATURES VECTOR
    # ========================================================================
    print("\n" + "=" * 80)
    print("B∆Ø·ªöC 4: PREPARE FEATURES VECTOR")
    print("=" * 80)
    
    # Ch·ªçn c√°c features ƒë·ªÉ train
    feature_cols = [
        # Base features
        'age_years', 'gender', 'height', 'weight',
        'ap_hi', 'ap_lo', 'cholesterol', 'gluc',
        'smoke', 'alco', 'active', 'bmi', 'pulse_pressure',
        # Engineered features
        'bmi_age_interaction', 'bp_index', 
        'lifestyle_risk_score', 'health_score',
        'height_weight_ratio', 'age_bin', 'bmi_bin', 'bp_bin'
    ]
    
    logger.info(f"üìä T·ªïng s·ªë features: {len(feature_cols)}")
    logger.info(f"   Features: {', '.join(feature_cols)}")
    
    # Assemble features th√†nh vector
    from pyspark.ml.feature import VectorAssembler
    
    assembler = VectorAssembler(
        inputCols=feature_cols,
        outputCol="features",
        handleInvalid="skip"
    )
    
    df_with_features = assembler.transform(df_ml)
    
    # Select ch·ªâ features v√† target
    df_final = df_with_features.select("features", "cardio")
    
    logger.info(f"‚úì ƒê√£ t·∫°o feature vector")
    logger.info(f"‚úì D·ªØ li·ªáu cu·ªëi: {df_final.count():,} records")
    
    # Cache ƒë·ªÉ tƒÉng t·ªëc
    df_final.cache()
    
    # ========================================================================
    # B∆Ø·ªöC 5: TRAIN V·ªöI K-FOLD CROSS VALIDATION
    # ========================================================================
    print("\n" + "=" * 80)
    print("B∆Ø·ªöC 5: TRAINING V·ªöI K-FOLD CROSS VALIDATION")
    print("=" * 80)
    
    # T·∫°o model instance
    cardio_model = CardioLogisticModel(spark)
    
    # ƒê·ªãnh nghƒ©a param grid ƒë·ªÉ test
    param_grid = {
        'maxIter': [50, 100, 150],
        'regParam': [0.001, 0.01, 0.1],
        'elasticNetParam': [0.0, 0.5, 1.0]
    }
    
    # Ch·∫°y pipeline ƒë·∫ßy ƒë·ªß: Split ‚Üí K-fold CV ‚Üí Evaluate
    results = cardio_model.train_with_cv_pipeline(
        df=df_final,
        train_ratio=0.8,
        param_grid=param_grid,
        num_folds=5,
        seed=42
    )
    
    # ========================================================================
    # B∆Ø·ªöC 6: PH√ÇN T√çCH K·∫æT QU·∫¢
    # ========================================================================
    print("\n" + "=" * 80)
    print("B∆Ø·ªöC 6: PH√ÇN T√çCH K·∫æT QU·∫¢ CHI TI·∫æT")
    print("=" * 80)
    
    # Best parameters
    print("\nüéØ BEST HYPERPARAMETERS:")
    for param, value in results['best_params'].items():
        print(f"  {param}: {value}")
    
    # CV metrics
    print("\nüìä CROSS VALIDATION METRICS:")
    print(f"  Best CV AUC: {results['cv_metrics']['best_auc']:.4f}")
    print(f"  Num folds:   {results['cv_metrics']['num_folds']}")
    
    # Test metrics
    print("\nüìà TEST SET METRICS (Unseen Data):")
    test_metrics = results['test_metrics']
    print(f"  Accuracy:  {test_metrics['accuracy']:.4f}")
    print(f"  Precision: {test_metrics['precision']:.4f}")
    print(f"  Recall:    {test_metrics['recall']:.4f}")
    print(f"  F1-Score:  {test_metrics['f1_score']:.4f}")
    print(f"  AUC-ROC:   {test_metrics['auc_roc']:.4f}")
    
    # Confusion Matrix
    cm = test_metrics['confusion_matrix']
    print("\nüìã CONFUSION MATRIX:")
    print(f"              Predicted")
    print(f"              No    Yes")
    print(f"  Actual No   {cm['TN']:>5} {cm['FP']:>5}")
    print(f"  Actual Yes  {cm['FN']:>5} {cm['TP']:>5}")
    
    # Calculate additional metrics
    sensitivity = cm['TP'] / (cm['TP'] + cm['FN']) if (cm['TP'] + cm['FN']) > 0 else 0
    specificity = cm['TN'] / (cm['TN'] + cm['FP']) if (cm['TN'] + cm['FP']) > 0 else 0
    
    print(f"\n  Sensitivity (Recall):  {sensitivity:.4f}")
    print(f"  Specificity:           {specificity:.4f}")
    
    # ========================================================================
    # B∆Ø·ªöC 7: FEATURE IMPORTANCE
    # ========================================================================
    print("\n" + "=" * 80)
    print("B∆Ø·ªöC 7: FEATURE IMPORTANCE")
    print("=" * 80)
    
    importance_df = cardio_model.get_feature_importance(feature_cols)
    
    print("\nüîù TOP 10 IMPORTANT FEATURES:")
    print(importance_df.head(10).to_string(index=False))
    
    # ========================================================================
    # B∆Ø·ªöC 8: SAVE MODEL
    # ========================================================================
    print("\n" + "=" * 80)
    print("B∆Ø·ªöC 8: L∆ØU MODEL")
    print("=" * 80)
    
    cardio_model.save_model()
    logger.info(f"‚úì Model ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i: {config.MODEL_PATH}")
    
    # ========================================================================
    # B∆Ø·ªöC 9: TEST PREDICTION
    # ========================================================================
    print("\n" + "=" * 80)
    print("B∆Ø·ªöC 9: TEST PREDICTION V·ªöI M·ªòT SAMPLE")
    print("=" * 80)
    
    # L·∫•y m·ªôt sample t·ª´ test set
    sample = results['test_df'].limit(1).select("features", "cardio").collect()[0]
    sample_features = sample["features"].toArray().tolist()
    actual_label = sample["cardio"]
    
    # Predict
    prediction_result = cardio_model.predict_single(sample_features)
    
    print(f"\nüìù Sample Prediction:")
    print(f"  Actual label:     {actual_label} ({'C√≥ b·ªánh' if actual_label == 1 else 'Kh√¥ng b·ªánh'})")
    print(f"  Predicted:        {prediction_result['prediction']} ({prediction_result['prediction_label']})")
    print(f"  Probability (No): {prediction_result['probability_negative']:.4f}")
    print(f"  Probability (Yes): {prediction_result['probability_positive']:.4f}")
    
    # ========================================================================
    # K·∫æT TH√öC
    # ========================================================================
    print("\n" + "=" * 80)
    print("‚úÖ HO√ÄN TH√ÄNH TO√ÄN B·ªò PIPELINE!")
    print("=" * 80)
    
    print("\nüìÅ C√ÅC FILE ƒê√É T·∫†O:")
    print(f"  - Processed data: {config.PROCESSED_DATA_FILE}")
    print(f"  - Model:          {config.MODEL_PATH}")
    
    print("\nüéâ Pipeline ho√†n th√†nh th√†nh c√¥ng!")
    print("=" * 80 + "\n")
    
    # Cleanup
    df_final.unpersist()
    
    return results


if __name__ == "__main__":
    try:
        results = main()
        
        # Optional: Save results to file
        import json
        import numpy as np
        
        # Convert results to JSON-serializable format
        results_summary = {
            'best_params': results['best_params'],
            'cv_metrics': {
                'best_auc': float(results['cv_metrics']['best_auc']),
                'num_folds': results['cv_metrics']['num_folds']
            },
            'test_metrics': {
                k: float(v) if isinstance(v, (int, float, np.number)) else v 
                for k, v in results['test_metrics'].items()
                if k != 'confusion_matrix'
            },
            'confusion_matrix': results['test_metrics']['confusion_matrix']
        }
        
        # Save to file
        with open('results/pipeline_results.json', 'w') as f:
            json.dump(results_summary, f, indent=2)
        
        print("‚úì K·∫øt qu·∫£ ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o: results/pipeline_results.json")
        
    except Exception as e:
        logger.error(f"‚ùå L·ªói khi ch·∫°y pipeline: {str(e)}", exc_info=True)
        sys.exit(1)
    
    finally:
        # Stop Spark
        SparkManager.stop()